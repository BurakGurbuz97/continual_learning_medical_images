{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amrit/anaconda3/envs/remind_proj/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Accuracy:  0.0\n",
      "Average Accuracy:  0.5\n",
      "Average Accuracy:  0.75\n",
      "After reset:  0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from avalanche.evaluation.metrics import Accuracy, TaskAwareAccuracy , BWT\n",
    "\n",
    "# create an instance of the standalone Accuracy metric\n",
    "# initial accuracy is 0\n",
    "acc_metric = Accuracy()\n",
    "print(\"Initial Accuracy: \", acc_metric.result()) #  output 0.0\n",
    "\n",
    "# two consecutive metric updates\n",
    "real_y = torch.tensor([1, 2]).long()\n",
    "predicted_y = torch.tensor([1, 0]).float()\n",
    "acc_metric.update(real_y, predicted_y)\n",
    "acc = acc_metric.result()\n",
    "print(\"Average Accuracy: \", acc) # output 0.5\n",
    "predicted_y = torch.tensor([1,2]).float()\n",
    "acc_metric.update(real_y, predicted_y)\n",
    "acc = acc_metric.result()\n",
    "print(\"Average Accuracy: \", acc) # output 0.75\n",
    "\n",
    "# reset accuracy\n",
    "acc_metric.reset()\n",
    "print(\"After reset: \", acc_metric.result()) # output 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Accuracy:  {}\n",
      "Average Accuracy:  {0: 0.5}\n",
      "Average Accuracy:  {0: 0.5, 1: 1.0}\n",
      "Average Accuracy:  {0: 0.75, 1: 1.0}\n",
      "Average Accuracy:  {0: 0.75, 1: 1.0, 2: 1.0}\n",
      "Average Accuracy:  {0: 0.75, 1: 1.0, 2: 1.0, 3: 1.0}\n",
      "After reset:  {}\n"
     ]
    }
   ],
   "source": [
    "from avalanche.evaluation.metrics import Accuracy, TaskAwareAccuracy , BWT\n",
    "\n",
    "# create an instance of the standalone TaskAwareAccuracy metric\n",
    "# initial accuracy is 0 for each task\n",
    "acc_metric = TaskAwareAccuracy()\n",
    "print(\"Initial Accuracy: \", acc_metric.result()) #  output {}\n",
    "\n",
    "# metric updates for 2 different tasks\n",
    "task_label = 0\n",
    "real_y = torch.tensor([1, 2]).long()\n",
    "predicted_y = torch.tensor([1, 0]).float()\n",
    "acc_metric.update(real_y, predicted_y, task_label)\n",
    "acc = acc_metric.result()\n",
    "print(\"Average Accuracy: \", acc) # output 0.5 for task 0\n",
    "\n",
    "task_label = 1\n",
    "predicted_y = torch.tensor([1,2]).float()\n",
    "acc_metric.update(real_y, predicted_y, task_label)\n",
    "acc = acc_metric.result() \n",
    "print(\"Average Accuracy: \", acc) # output 0.75 for task 0 and 1.0 for task 1\n",
    "\n",
    "task_label = 0\n",
    "predicted_y = torch.tensor([1,2]).float()\n",
    "acc_metric.update(real_y, predicted_y, task_label)\n",
    "acc = acc_metric.result()\n",
    "print(\"Average Accuracy: \", acc) # output 0.75 for task 0 and 1.0 for task 1\n",
    "\n",
    "task_label = 2\n",
    "predicted_y = torch.tensor([1,2]).float()\n",
    "acc_metric.update(real_y, predicted_y, task_label)\n",
    "acc = acc_metric.result()\n",
    "print(\"Average Accuracy: \", acc) # output 0.75 for task 0 and 1.0 for task 1\n",
    "\n",
    "task_label = 3\n",
    "predicted_y = torch.tensor([1,2]).float()\n",
    "acc_metric.update(real_y, predicted_y, task_label)\n",
    "acc = acc_metric.result()\n",
    "print(\"Average Accuracy: \", acc) # output 0.75 for task 0 and 1.0 for task 1\n",
    "\n",
    "\n",
    "# reset accuracy\n",
    "acc_metric.reset()\n",
    "print(\"After reset: \", acc_metric.result()) # output {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy:  {}\n",
      "Average Accuracy:  {tensor([1, 2]): tensor([-0., 2.])}\n",
      "Average Accuracy:  {tensor([1, 2]): tensor([-0., 2.])}\n"
     ]
    }
   ],
   "source": [
    "from avalanche.evaluation.metrics import Accuracy, TaskAwareAccuracy , BWT\n",
    "\n",
    "# create an instance of the standalone TaskAwareAccuracy metric\n",
    "# initial accuracy is 0 for each task\n",
    "acc_metric = BWT()\n",
    "# metric updates for 2 different tasks\n",
    "task_label = 0\n",
    "real_y = torch.tensor([1, 2]).long()\n",
    "predicted_y = torch.tensor([1, 0]).float()\n",
    "acc_metric.update_initial(real_y, predicted_y)\n",
    "acc = acc_metric.result()\n",
    "print(\"Average Accuracy: \", acc) # output 0.5 for task 0\n",
    "\n",
    "task_label = 1\n",
    "predicted_y = torch.tensor([1,2]).float()\n",
    "acc_metric.update(real_y, predicted_y)\n",
    "acc = acc_metric.result() \n",
    "print(\"Average Accuracy: \", acc) # output 0.75 for task 0 and 1.0 for task 1\n",
    "\n",
    "task_label = 0\n",
    "predicted_y = torch.tensor([1,2]).float()\n",
    "acc_metric.update_last(real_y, predicted_y)\n",
    "acc = acc_metric.result()\n",
    "print(\"Average Accuracy: \", acc) # output 0.75 for task 0 and 1.0 for task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.56922006, 0.        , 0.        ])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = [[0.5692200557103064,0,0],\n",
    "[0.011002785515320334,0.5647471499561532,0],\n",
    "[0.0,0.0,0.4970389170896785]]\n",
    "\n",
    "import numpy as np\n",
    "acc = np.array(acc)\n",
    "\n",
    "acc[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BWT for Task 2: {0: 0.07000000000000006, 1: 0.07000000000000006, 2: 0.07000000000000006}\n"
     ]
    }
   ],
   "source": [
    "from avalanche.evaluation.metrics import BWT \n",
    "\n",
    "# Simulate some example results\n",
    "# Suppose you have three tasks and their accuracies in a continual learning scenario\n",
    "# where each task is trained one after another\n",
    "task1_accuracy = 0.9\n",
    "task2_accuracy = 0.85\n",
    "task3_accuracy = 0.92\n",
    "bwt = BWT()\n",
    "\n",
    "# Compute BWT for each task\n",
    "bwt.update(0, task2_accuracy , initial=True)\n",
    "bwt.update(1, task2_accuracy , initial=True)\n",
    "bwt.update(2, task2_accuracy , initial=True)\n",
    "\n",
    "bwt.update(0, task3_accuracy )\n",
    "bwt.update(1, task3_accuracy )\n",
    "bwt.update(0, task3_accuracy )\n",
    "bwt.update(1, task3_accuracy )\n",
    "bwt.update(2 , task3_accuracy)\n",
    "\n",
    "print(\"BWT for Task 2:\", bwt.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.1'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import avalanche\n",
    "avalanche.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amrit/anaconda3/envs/remind_proj/lib/python3.7/site-packages/ipykernel_launcher.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BWT': {0: -0.49, 1: -0.16000000000000003, 2: 0.3},\n",
       " 'ForwardTransfer': {0: -0.49, 1: -0.16000000000000003, 2: 0.3},\n",
       " 'Forgetting': {0: 0.49, 1: 0.16000000000000003, 2: -0.3}}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from avalanche.evaluation.metrics import BWT , ForwardTransfer , Forgetting\n",
    "import numpy as np\n",
    "\n",
    "acc = [[0.64],\n",
    "[0.23,0.76],\n",
    "[0.15 , 0.10 , 0.44],\n",
    "[0.15 , 0.60 , 0.74,0.46]]\n",
    "\n",
    "acc = np.array(acc)\n",
    "\n",
    "bwt = BWT()\n",
    "bwt = ForwardTransfer()\n",
    "bwt = Forgetting()\n",
    "bwt = ForwardTransfer()\n",
    "\n",
    "result = {}\n",
    "for metric_function in [BWT, ForwardTransfer, Forgetting]:\n",
    "    metric = metric_function()\n",
    "        \n",
    "    for x in range(acc.shape[0]):\n",
    "        for y in range(len(acc[x])):\n",
    "            # print(x, y , acc[x][y])\n",
    "            if y == x:\n",
    "                metric.update(y, acc[x][y] , initial=True)\n",
    "            else:\n",
    "                metric.update(y, acc[x][y] )\n",
    "\n",
    "    result[str(metric.__class__).split(\".\")[-1][:-2]] = metric.result()\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"BWT'>\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(metric.__class__).split(\".\")[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 0 -0.16899999999999993 0.4 0.569\n",
      "\n",
      "2 0 0 -0.33899999999999997 0.23 0.569\n",
      "2 1 -0.33899999999999997 -0.64 0.0 0.64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.16899999999999993, -0.979], -0.38266666666666665)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = [[0.569,0,0],\n",
    "[0.40,0.64,0],\n",
    "[0.23,0.0,0.497]]\n",
    "\n",
    "import numpy as np\n",
    "acc = np.array(acc)\n",
    "\n",
    "sum_total = []\n",
    "N = 3\n",
    "for i in range(1,N):\n",
    "    sum1 = 0\n",
    "    for j in range(0,i):\n",
    "        aaa = acc[i][j] - acc[j][j]\n",
    "        print(i, j ,sum1, aaa , acc[i][j] , acc[j][j])\n",
    "        sum1 += aaa\n",
    "    sum1 = sum1 \n",
    "    sum_total.append(sum1)\n",
    "    print()\n",
    "    \n",
    "sum_total, sum(sum_total)/(N* (N-1)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 0 -0.16899999999999993 0.4 0.569\n",
      "\n",
      "2 0 0 -0.33899999999999997 0.23 0.569\n",
      "2 1 -0.33899999999999997 -0.64 0.0 0.64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([-0.16899999999999993, -0.979], -0.38266666666666665)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = [[0.569,0,0],\n",
    "[0.40,0.64,0],\n",
    "[0.23,0.0,0.497]]\n",
    "\n",
    "# acc = [[0.569,0,0],\n",
    "# [0.10,0.64,0]]\n",
    "\n",
    "import numpy as np\n",
    "acc = np.array(acc)\n",
    "\n",
    "sum_total = []\n",
    "N = 3\n",
    "for i in range(1,N):\n",
    "    sum1 = 0\n",
    "    for j in range(0,i):\n",
    "        aaa = acc[i][j] - acc[j][j]\n",
    "        print(i, j ,sum1, aaa , acc[i][j] , acc[j][j])\n",
    "        sum1 += aaa\n",
    "    sum1 = sum1 \n",
    "    sum_total.append(sum1)\n",
    "    print()\n",
    "    \n",
    "sum_total, sum(sum_total)/(N* (N-1)/2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward and backward transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BWT [-0.33899999999999997, -0.64, 0.0, -0.4895]\n",
      "FWT [0.0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_forward_transfer(acc, b ):\n",
    "    score = []\n",
    "    N = len(acc)\n",
    "    for i in range(1,N):\n",
    "        aaa = acc[i-1][i] - b[i]\n",
    "        # print(i, aaa , acc[i-1][i])\n",
    "    score.append(sum(score)/(N-1))\n",
    "    return score\n",
    "\n",
    "def get_backward_transfer(acc):\n",
    "    score = []\n",
    "    N = len(acc)\n",
    "    for i in range(0,N):\n",
    "        aaa = acc[N-1][i] - acc[i][i]\n",
    "        # print(i, aaa , acc[N-1][i] , acc[i][i])\n",
    "        score.append(aaa)\n",
    "    score.append(sum(score)/(N-1))\n",
    "    return score\n",
    "\n",
    "acc = [[0.569,0.05, 0 ],\n",
    "[0.40,0.64,0.07],\n",
    "[0.23,0.0,0.497]]\n",
    "\n",
    "naive_acc = [[0.7127919135587313,0.0,0.0,0.0,0.0,0.0],\n",
    "       [0.0,0.9716284492809949,0.0,0.0,0.0,0.0],\n",
    "       [0.0,0.24446171783909834,0.9747292418772563,0.0,0.0,0.0],\n",
    "        [0.0,0.0,0.4693140794223827,0.8364377182770664,0.0,0.0],\n",
    "        [0.0,0.0,0.5324909747292419,0.3940628637951106,0.7243589743589743,0.0],\n",
    "        [0.0,0.0,0.0,0.002328288707799767,0.0,0.625]]\n",
    "\n",
    "df = pd.read_csv(\"./pathology/results_NISPA0_orig_Microscopic_CIL.csv\")\n",
    "df = df.drop(columns=[\"Unnamed: 0\"]).fillna(0).to_numpy()\n",
    "# df = np.where(df==np.nan)\n",
    "print(df)\n",
    "\n",
    "print(\"BWT\" , get_backward_transfer(df))\n",
    "print(\"FWT\" , get_forward_transfer(df, [0]*len(df)))\n",
    "\n",
    "\n",
    "print(\"BWT\" , get_backward_transfer(acc))\n",
    "print(\"FWT\" , get_forward_transfer(acc, [0]*len(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.569, 0.05, 0], [0.4, 0.64, 0.07], [0.23, 0.0, 0.497]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['task_wise', 'average']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list( get_forward_transfer(acc, [0]*len(acc)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results file to latex table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_to_latex(matrix, caption, align='|c'):\n",
    "    rows = len(matrix)\n",
    "    cols = len(matrix[0])\n",
    "\n",
    "    latex_code = \"\\\\begin{table}\\n\" #[htbp]\n",
    "    latex_code += \"\\\\centering\\n\"\n",
    "    latex_code += \"\\\\begin{tabular}{\" + align * cols + \"|\" + \"}\\n\"\n",
    "    \n",
    "    for row in matrix:\n",
    "        latex_code += \" & \".join(str(entry) for entry in row) + \" \\\\\\\\\\n \\hline \\n\" \n",
    "    \n",
    "    latex_code += \"\\\\end{tabular}\\n\"\n",
    "    latex_code += \"\\\\caption{Table: \" + caption+ \"}\\n\"  # Replace with your table caption\n",
    "    latex_code += \"\\\\label{tab:\" + caption+ \"}\\n\"  # Replace with your table label\n",
    "    latex_code += \"\\\\end{table}\"\n",
    "\n",
    "    return latex_code\n",
    "\n",
    "\n",
    "def file_to_latex(filename, caption):\n",
    "    # Read matrix from text file\n",
    "    matrix = []\n",
    "    first_row = None\n",
    "    with open(filename, 'r') as file:\n",
    "        \n",
    "        for line in file:\n",
    "            row = line.strip().split(',')\n",
    "            if first_row == None:\n",
    "                first_row = row\n",
    "\n",
    "            new_row = []\n",
    "            for entry in row:\n",
    "                if entry.replace('.', '', 1).isdigit():\n",
    "                    entry = str(round(float(entry), 4))\n",
    "                if \"Eval After\" in entry:\n",
    "                    entry = entry.replace('Eval After', '', 1)\n",
    "                if \"Acc\" in entry:\n",
    "                    entry = entry.replace('Acc', '', 1)\n",
    "                new_row.append(entry)\n",
    "            row = new_row\n",
    "            \n",
    "            # row = [entry for entry in row ] #if entry.replace('.', '', 1).isdigit()\n",
    "            matrix.append(row)\n",
    "\n",
    "    # Convert matrix to LaTeX table code\n",
    "    latex = matrix_to_latex(matrix, caption)\n",
    "\n",
    "    # Print the LaTeX code\n",
    "    print(latex)\n",
    "\n",
    "\n",
    "# file_to_latex(filename , caption)\n",
    "import glob\n",
    "files_list = glob.glob(\"/home/amrit/pipeline/in_progress/continual_learning/Deep-learning-project/results/may_29/*.csv\")\n",
    "\n",
    "                \n",
    "for filename in files_list:\n",
    "    file_to_latex(filename, filename.split(\"/\")[-1][:-4].replace('_', ' ') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_to_latex(matrix, align='c'):\n",
    "    rows = len(matrix)\n",
    "    cols = len(matrix[0])\n",
    "\n",
    "    latex_code = \"\\\\begin{table}[htbp]\\n\"\n",
    "    latex_code += \"\\\\centering\\n\"\n",
    "    latex_code += \"\\\\begin{tabular}{\" + align * cols + \"}\\n\"\n",
    "    \n",
    "    for row in matrix:\n",
    "        latex_code += \" & \".join(str(entry) for entry in row) + \" \\\\\\\\\\n\"\n",
    "    \n",
    "    latex_code += \"\\\\end{tabular}\\n\"\n",
    "    latex_code += \"\\\\caption{Table Caption}\\n\"  # Replace with your table caption\n",
    "    latex_code += \"\\\\label{table:my_table}\\n\"  # Replace with your table label\n",
    "    latex_code += \"\\\\end{table}\"\n",
    "\n",
    "    return latex_code\n",
    "\n",
    "# Example usage\n",
    "filename = \"matrix.txt\"  # Replace with your file path\n",
    "filename = \"results_Radio_CIL_NAIVE_LEARNER.csv\"  # Replace with your file path\n",
    "\n",
    "# Read matrix from text file\n",
    "with open(filename, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    matrix = [list(map(float, line.strip().split())) for line in lines]\n",
    "\n",
    "# Convert matrix to LaTeX table code\n",
    "latex = matrix_to_latex(matrix)\n",
    "\n",
    "# Print the LaTeX code\n",
    "print(latex)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "remind_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
